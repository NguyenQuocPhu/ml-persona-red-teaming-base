import os
import multiprocessing
from typing import List, Any, Dict
# LÆ°u Ã½: KHÃ”NG import vllm á»Ÿ Ä‘Ã¢y Ä‘á»ƒ trÃ¡nh khá»Ÿi táº¡o CUDA sai á»Ÿ main process
from rainbowplus.llms.base import BaseLLM

# --- HÃ€M WORKER CHáº Y TRONG SANDBOX RIÃŠNG BIá»†T ---
def _vllm_worker_process(model_kwargs, device_id, input_q, output_q):
    """
    HÃ m nÃ y cháº¡y trong má»™t Process hoÃ n toÃ n má»›i.
    Táº¡i Ä‘Ã¢y ta cÃ³ thá»ƒ set biáº¿n mÃ´i trÆ°á»ng an toÃ n tuyá»‡t Ä‘á»‘i.
    """
    try:
        # 1. THIáº¾T Láº¬P MÃ”I TRÆ¯á»œNG CÃ” Láº¬P GPU
        # BÆ°á»›c nÃ y pháº£i lÃ m TRÆ¯á»šC KHI import vllm/torch
        if device_id is not None:
            os.environ["CUDA_VISIBLE_DEVICES"] = str(device_id)
            print(f"ğŸ”’ [Child Process] Masked GPU: Visible Devices = {device_id}")
        
        # 2. BÃ¢y giá» má»›i import vLLM (Ä‘á»ƒ nÃ³ nháº­n diá»‡n mÃ´i trÆ°á»ng má»›i)
        from vllm import LLM, SamplingParams
        
        # 3. Khá»Ÿi táº¡o Engine
        llm = LLM(**model_kwargs)
        print(f"âœ… [Child Process] vLLM initialized successfully on {device_id}")
        
        # 4. VÃ²ng láº·p láº¯ng nghe vÃ  xá»­ lÃ½ yÃªu cáº§u
        while True:
            task = input_q.get()
            if task is None: # TÃ­n hiá»‡u dá»«ng
                break
            
            req_id, method, args = task
            
            try:
                result = None
                if method == 'generate':
                    # args: (query, sampling_params)
                    query, params_dict = args
                    outputs = llm.generate([query], SamplingParams(**params_dict))
                    if outputs:
                        result = outputs[0].outputs[0].text
                    else:
                        result = ""
                        
                elif method == 'batch_generate':
                    # args: (queries, sampling_params)
                    queries, params_dict = args
                    outputs = llm.generate(queries, SamplingParams(**params_dict))
                    result = [o.outputs[0].text for o in outputs]
                
                output_q.put((req_id, result, None)) # (ID, Data, Error)
                
            except Exception as inner_e:
                output_q.put((req_id, None, inner_e))
                
    except Exception as e:
        # Náº¿u khá»Ÿi táº¡o tháº¥t báº¡i, gá»­i lá»—i vá» main process
        # DÃ¹ng vÃ²ng láº·p Ä‘á»ƒ xáº£ queue náº¿u cáº§n, nhÆ°ng á»Ÿ Ä‘Ã¢y ta gá»­i lá»—i fatal
        # LÆ°u Ã½: Main process cáº§n cÆ¡ cháº¿ timeout Ä‘á»ƒ khÃ´ng bá»‹ treo náº¿u worker cháº¿t sá»›m
        print(f"âŒ [Child Process] Critical Error: {e}")
        pass

# --- CLASS WRAPPER CHÃNH ---
class vLLM(BaseLLM):
    def __init__(self, model_kwargs: dict):
        self.model_kwargs = model_kwargs.copy()
        
        # Láº¥y device tá»« config (VD: "0" hoáº·c "1")
        self.device = self.model_kwargs.pop("device", None)
        
        # Kiá»ƒm tra Tensor Parallel (Ä‘á»ƒ xá»­ lÃ½ Fitness Model cháº¡y nhiá»u GPU)
        tp_size = self.model_kwargs.get("tensor_parallel_size", 1)
        if tp_size > 1 and self.device is None:
            # Náº¿u cháº¡y TP mÃ  khÃ´ng chá»‰ Ä‘á»‹nh device, ta giáº£ Ä‘á»‹nh dÃ¹ng táº¥t cáº£
            # Hoáº·c báº¡n cÃ³ thá»ƒ set self.device = "0,1" trong config
            pass

        # Sá»­ dá»¥ng 'spawn' context Ä‘á»ƒ Ä‘áº£m báº£o process má»›i sáº¡ch sáº½ (quan trá»ng cho CUDA)
        ctx = multiprocessing.get_context('spawn')
        self.input_queue = ctx.Queue()
        self.output_queue = ctx.Queue()
        
        print(f"ğŸš€ [Main Process] Spawning isolated vLLM worker for device: {self.device}")
        
        self.process = ctx.Process(
            target=_vllm_worker_process,
            args=(self.model_kwargs, self.device, self.input_queue, self.output_queue)
        )
        self.process.start()
        
        # Biáº¿n Ä‘áº¿m request Ä‘á»ƒ map káº¿t quáº£ tráº£ vá»
        self.req_counter = 0

    def _send_and_wait(self, method, args):
        """Gá»­i yÃªu cáº§u sang process con vÃ  Ä‘á»£i káº¿t quáº£"""
        if not self.process.is_alive():
            raise RuntimeError("vLLM Worker Process is dead!")
            
        req_id = self.req_counter
        self.req_counter += 1
        
        self.input_queue.put((req_id, method, args))
        
        # Äá»£i káº¿t quáº£ (Blocking)
        r_id, result, error = self.output_queue.get()
        
        if error:
            raise error
        return result

    def get_name(self):
        return self.model_kwargs.get("model", "isolated-vllm")

    def generate(self, query: str, sampling_params: dict):
        return self._send_and_wait('generate', (query, sampling_params))

    def batch_generate(self, queries: List[str], sampling_params: dict):
        return self._send_and_wait('batch_generate', (queries, sampling_params))

    def __del__(self):
        # Dá»n dáº¹p process khi object bá»‹ há»§y
        if hasattr(self, 'process') and self.process.is_alive():
            self.input_queue.put(None) # Gá»­i tÃ­n hiá»‡u dá»«ng
            self.process.join(timeout=5)
            if self.process.is_alive():
                self.process.terminate()