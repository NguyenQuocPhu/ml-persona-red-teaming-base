import time
import logging
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from google.api_core.exceptions import ResourceExhausted, InternalServerError
from typing import Any, Dict, List

# Giá»¯ nguyÃªn import BaseLLM
try:
    from rainbowplus.llms.base import BaseLLM
except ImportError:
    class BaseLLM: 
        def get_name(self): pass
        def batch_generate(self): pass

logger = logging.getLogger(__name__)

class GeminiLLM(BaseLLM):
    def __init__(self, config: Any):
        super().__init__()
        self.config = config

        model_kwargs = getattr(config, "model_kwargs", {}) or {}
        
        # 1. Cáº¥u hÃ¬nh API Key & Model
        self.api_key = model_kwargs.get("api_key")
        # Tá»± Ä‘á»™ng xÃ³a prefix 'models/' náº¿u cÃ³
        raw_model_name = model_kwargs.get("model", "gemini-1.5-flash")
        self.model_name = raw_model_name.replace("models/", "")
        
        # 2. Cáº¥u hÃ¬nh Rate Limit (RPM)
        self.rpm = model_kwargs.get("rpm", 5) 
        self.min_interval = 60.0 / float(self.rpm)
        self.last_call_time = 0.0

        if self.api_key:
            genai.configure(api_key=self.api_key)
        else:
            logger.error("âŒ ERROR: Thiáº¿u 'api_key' trong config!")

        self._model = None

    def _ensure_model_client(self):
        if self._model is None and self.api_key:
            self._model = genai.GenerativeModel(self.model_name)

    def get_name(self) -> str:
        return self.model_name

    def _wait_for_rate_limit(self):
        """Chá»§ Ä‘á»™ng ngá»§ Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng vÆ°á»£t quÃ¡ RPM"""
        now = time.time()
        elapsed = now - self.last_call_time
        if elapsed < self.min_interval:
            sleep_time = self.min_interval - elapsed
            time.sleep(sleep_time)

    def generate(self, prompt: str, sampling_params: Dict[str, Any] = None, max_retries: int = 5) -> str:
        if not self.api_key: return ""
        self._ensure_model_client()

        default_params = dict(getattr(self.config, "sampling_params", {}) or {})
        if sampling_params: default_params.update(sampling_params)
        
        gen_config = genai.GenerationConfig(
            temperature=default_params.get("temperature", 0.7),
            max_output_tokens=default_params.get("max_tokens", 1024),
            top_p=default_params.get("top_p", 0.9)
        )

        # --- QUAN TRá»ŒNG: Táº¯t bá»™ lá»c an toÃ n (BLOCK_NONE) ---
        safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        # ---------------------------------------------------

        for attempt in range(max_retries):
            try:
                self._wait_for_rate_limit() # 1. Chá» RPM

                # Gá»i API vá»›i safety_settings
                response = self._model.generate_content(
                    prompt, 
                    generation_config=gen_config,
                    safety_settings=safety_settings
                )
                
                self.last_call_time = time.time() # 2. Cáº­p nháº­t thá»i gian

                # Kiá»ƒm tra pháº£n há»“i an toÃ n
                if response.candidates and response.candidates[0].content.parts:
                    return response.text
                elif response.prompt_feedback and response.prompt_feedback.block_reason:
                    logger.warning(f"âš ï¸ Prompt blocked by Google: {response.prompt_feedback.block_reason}")
                    return "I cannot answer." # Tráº£ vá» text giáº£ Ä‘á»ƒ code khÃ´ng crash
                else:
                    return ""

            except ResourceExhausted:
                # Lá»—i 429: QuÃ¡ táº£i -> Ngá»§ lÃ¢u hÆ¡n
                wait_time = (2 ** attempt) + 2
                logger.warning(f"âš ï¸ Rate Limit (429). Waiting {wait_time}s...")
                time.sleep(wait_time)
                self.last_call_time = time.time()

            except InternalServerError:
                time.sleep(2)
            
            except Exception as e:
                # Báº¯t lá»—i Invalid operation (náº¿u safety filter váº«n lá»t lÆ°á»›i)
                if "Invalid operation" in str(e) or "finish_reason" in str(e):
                    logger.warning("âš ï¸ Safety Filter Blocked (Finish Reason 2). Returning empty.")
                    return "I cannot answer."
                
                logger.error(f"Generate Error: {e}")
                break
        
        return ""

    def batch_generate(self, prompts: List[str], sampling_params: Dict[str, Any] = None) -> List[str]:
        """
        Xá»­ lÃ½ danh sÃ¡ch prompt tuáº§n tá»± Ä‘á»ƒ Ä‘áº£m báº£o an toÃ n Rate Limit.
        """
        results = []
        total = len(prompts)
        logger.info(f"ğŸš€ Báº¯t Ä‘áº§u batch generate cho {total} prompts vá»›i model {self.model_name}...")
        
        for i, prompt in enumerate(prompts):
            res = self.generate(prompt, sampling_params)
            results.append(res)
            
            if (i + 1) % 10 == 0:
                logger.info(f"   ...ÄÃ£ xá»­ lÃ½ {i + 1}/{total} prompts.")
                
        return results